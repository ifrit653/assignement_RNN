{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J1-iLe4nS01T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "whoy1UV6jVH8"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z' ]\", \"\", text)  # Remove non-alphabetic characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespaces\n",
        "    return text\n",
        "def preprocess_sentences(sentences):\n",
        "    return [clean_text(sentence) for sentence in sentences]\n",
        "\n",
        "# Load and preprocess data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XNIMvxJ_S7Ls"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
        "        raw_lines = file.readlines()\n",
        "    # Filter blank lines and strip text\n",
        "    raw_lines = [line.strip() for line in raw_lines if line.strip()]\n",
        "    # Split English and French sentences\n",
        "    english_sentences = raw_lines[0::3]\n",
        "    french_sentences = raw_lines[1::3]\n",
        "    english_sentences = preprocess_sentences(english_sentences)\n",
        "    french_sentences = preprocess_sentences(french_sentences)\n",
        "    return english_sentences, french_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cx7o1KTwlJ6n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenize text\n",
        "def tokenize_sentences(sentences, max_vocab_size):\n",
        "    tokenizer = Tokenizer(num_words=max_vocab_size, filters='')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    return sequences, word_index, tokenizer\n",
        "\n",
        "# One-hot encode sentences\n",
        "def one_hot_encode(sequence, vocab_size):\n",
        "    return tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
        "\n",
        "# Load dataset\n",
        "path_translation = \"./data/opus-2019-12-04.test.txt\"\n",
        "english_sentences, french_sentences = load_data(path_translation)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6BPbX5Flytc",
        "outputId": "aaf4bb5e-f341-4a33-b403-d4ff1c9f4256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 413ms/step - accuracy: 0.7910 - loss: 3.7035\n",
            "Epoch 2/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 415ms/step - accuracy: 0.8511 - loss: 1.0733\n",
            "Epoch 3/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 406ms/step - accuracy: 0.8572 - loss: 0.9942\n",
            "Epoch 4/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 412ms/step - accuracy: 0.8638 - loss: 0.9371\n",
            "Epoch 5/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 417ms/step - accuracy: 0.8655 - loss: 0.9075\n",
            "Epoch 6/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 411ms/step - accuracy: 0.8681 - loss: 0.8759\n",
            "Epoch 7/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 387ms/step - accuracy: 0.8715 - loss: 0.8373\n",
            "Epoch 8/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 423ms/step - accuracy: 0.8748 - loss: 0.7979\n",
            "Epoch 9/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 406ms/step - accuracy: 0.8798 - loss: 0.7520\n",
            "Epoch 10/10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 419ms/step - accuracy: 0.8777 - loss: 0.7484\n",
            "Translation:                                                 \n"
          ]
        }
      ],
      "source": [
        "# Tokenize and preprocess\n",
        "max_vocab_en = 5000  # Limit vocabulary size\n",
        "max_vocab_fr = 5000\n",
        "english_sequences, word_index_en, tokenizer_en = tokenize_sentences(english_sentences, max_vocab_en)\n",
        "french_sequences, word_index_fr, tokenizer_fr = tokenize_sentences(french_sentences, max_vocab_fr)\n",
        "\n",
        "# Determine sequence lengths and pad sequences\n",
        "max_len_en = max(len(seq) for seq in english_sequences)\n",
        "max_len_fr = max(len(seq) for seq in french_sequences)\n",
        "max_len_fr = max(max_len_en, max_len_fr)\n",
        "# Ensure input and target sequences have the same length\n",
        "english_sequences = pad_sequences(english_sequences, maxlen=max_len_fr, padding='post')\n",
        "french_sequences = pad_sequences(french_sequences, maxlen=max_len_fr, padding='post')\n",
        "\n",
        "# Define simple RNN model\n",
        "class SimpleRNN(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=hidden_dim)\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(units=hidden_dim, return_sequences=True)\n",
        "        self.dense = tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.rnn(x)\n",
        "        output = self.dense(x)\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = len(word_index_en) + 1  # Vocabulary size of English\n",
        "output_dim = len(word_index_fr) + 1  # Vocabulary size of French\n",
        "hidden_dim = 128\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Create model\n",
        "model = SimpleRNN(input_dim, output_dim, hidden_dim)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare data for training\n",
        "input_data = np.array(english_sequences)\n",
        "target_data = np.array(french_sequences)  # No need to shift, it's already padded to the same length\n",
        "\n",
        "# Train model\n",
        "model.fit(input_data, target_data, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Testing and prediction\n",
        "def predict_translation(sentence, tokenizer, model, max_len):\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    prediction = model(sequence)\n",
        "    predicted_sequence = np.argmax(prediction, axis=-1)\n",
        "    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
        "    return \" \".join([reverse_word_index.get(idx, '') for idx in predicted_sequence[0]])\n",
        "\n",
        "# Example usage\n",
        "test_sentence = \"Hello\"\n",
        "translation = predict_translation(test_sentence, tokenizer_en, model, max_len_fr)\n",
        "print(f\"Translation: {translation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9Hg1KMblRcM",
        "outputId": "f3709c5d-3dbf-4b6c-cb51-427e96960597"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save the trained model to a file\n",
        "model.save('simple_rnn_translation_model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
